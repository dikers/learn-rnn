{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "import os\n",
    "import mxnet as mx\n",
    "from subprocess import call\n",
    "import random\n",
    "import math\n",
    "import zipfile\n",
    "from mxnet import autograd, nd, gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 2.2122064  -0.45138445  0.57938355 -1.856082   -1.9768796 ]\n",
       " [ 0.7740038  -0.20801921  0.2444218  -0.03716067 -0.48774993]\n",
       " [ 1.0434405  -0.02261727  0.57461417  1.4661262   0.6862904 ]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X, W_xh = nd.random.normal(shape=(3,1)) , nd.random.normal(shape=(1,4))\n",
    "H, W_hh = nd.random.normal(shape=(3,4)) , nd.random.normal(shape=(4,4))\n",
    "nd.concat(X, H, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.1839255   1.8917114  -1.2347414  -1.771029  ]\n",
       " [ 0.35496104  1.0731696   0.12017461 -0.9711102 ]\n",
       " [-0.77569664 -0.7882176   0.7417728  -1.4734439 ]\n",
       " [-1.0730928  -1.0424827  -1.3278849  -1.4749662 ]\n",
       " [-0.52414197  1.2662556   0.8950642  -0.6015945 ]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.concat(W_xh, W_hh, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 5.0373516   2.6754622  -1.6607479  -0.4062885 ]\n",
       " [ 0.94845396  0.46941754 -1.1866102  -1.1806769 ]\n",
       " [-1.1514019   0.8373027  -2.1974368  -5.2480164 ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(nd.concat(X, H, dim=1), nd.concat(W_xh, W_hh, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/dataset/jaychou_lyrics.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir  = 'datasets'\n",
    "output_dir = 'output'\n",
    "data_file = os.path.join(base_dir, \"jaychou_lyrics.txt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdf/workspace/git_hub_demo/learn-rnn\n",
      "/mnt/sdf/workspace/git_hub_demo/learn-rnn/datasets\n",
      "anna.txt\t\t     im2txt\t\t     spa.txt\n",
      "babi_tasks_1-20_v1-2.tar.gz  im2txt.zip\t\t     UCI HAR Dataset\n",
      "babi-tasks-v1-2.tar.gz\t     jaychou_lyrics.txt.zip  UCI HAR Dataset.zip\n",
      "cnews\t\t\t     mldata\n",
      "\n",
      "Downloading...\n",
      "Downloading done.\n",
      "\n",
      "/mnt/sdf/workspace/git_hub_demo/learn-rnn/datasets\n",
      "anna.txt\t\t     im2txt\t\t       mldata\n",
      "babi_tasks_1-20_v1-2.tar.gz  im2txt.zip\t\t       spa.txt\n",
      "babi-tasks-v1-2.tar.gz\t     jaychou_lyrics.txt.zip    UCI HAR Dataset\n",
      "cnews\t\t\t     jaychou_lyrics.txt.zip.1  UCI HAR Dataset.zip\n",
      "/mnt/sdf/workspace/git_hub_demo/learn-rnn\n"
     ]
    }
   ],
   "source": [
    "!pwd \n",
    "os.chdir(base_dir)\n",
    "!pwd && ls \n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Downloading...\")\n",
    "if not os.path.exists(data_file):\n",
    "    call(\n",
    "        'wget \"https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/dataset/jaychou_lyrics.txt.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Downloading done.\\n\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
    "\n",
    "\n",
    "!pwd && ls\n",
    "os.chdir(\"..\")\n",
    "!pwd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with zipfile.ZipFile(data_file) as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 63282 \n"
     ]
    }
   ],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "print('length: {} '.format(len(corpus_chars)))\n",
    "corpus_chars = corpus_chars[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars:  想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indices: [1010, 845, 66, 474, 406, 646, 970, 1010, 845, 713, 157, 443, 886, 680, 915, 441, 970, 1010, 845, 713]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars: ', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"Sample mini-batches in a random order from sequential data.\"\"\"\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos : pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i : i + batch_size]\n",
    "        X = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        Y = nd.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"Sample mini-batches in a consecutive order from sequential data.\"\"\"\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i : i + num_steps]\n",
    "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  \n",
      "[[ 0.  1.  2.  3.  4.]\n",
      " [20. 21. 22. 23. 24.]\n",
      " [40. 41. 42. 43. 44.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "Y:  \n",
      "[[ 1.  2.  3.  4.  5.]\n",
      " [21. 22. 23. 24. 25.]\n",
      " [41. 42. 43. 44. 45.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "\n",
      "X:  \n",
      "[[ 5.  6.  7.  8.  9.]\n",
      " [25. 26. 27. 28. 29.]\n",
      " [45. 46. 47. 48. 49.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "Y:  \n",
      "[[ 6.  7.  8.  9. 10.]\n",
      " [26. 27. 28. 29. 30.]\n",
      " [46. 47. 48. 49. 50.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "\n",
      "X:  \n",
      "[[10. 11. 12. 13. 14.]\n",
      " [30. 31. 32. 33. 34.]\n",
      " [50. 51. 52. 53. 54.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "Y:  \n",
      "[[11. 12. 13. 14. 15.]\n",
      " [31. 32. 33. 34. 35.]\n",
      " [51. 52. 53. 54. 55.]]\n",
      "<NDArray 3x5 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(60))\n",
    "for X, Y in data_iter_consecutive(my_seq, batch_size=3, num_steps=5):\n",
    "    print(\"X: \", X, '\\nY: ', Y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 1027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 1. 0. 0. 0.]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0,2]), vocab_size)\n",
    "print('vocab size {}'.format(vocab_size))\n",
    "nd.one_hot(nd.array([0,2]), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.arange(10).reshape((2, 5))\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "len(inputs), inputs[0].shape\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cpu(0)\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = try_gpu()\n",
    "print('will use', ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "    \n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    \n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    \n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    \n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "        \n",
    "    return outputs, (H, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X.shape (2, 5)  num_hiddens: 256 \n",
      "input shape: [\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>, \n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>, \n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>, \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>, \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<NDArray 2x1027 @cpu(0)>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), (2, 256))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\" X.shape {}  num_hiddens: {} \".format(X.shape , num_hiddens) )\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, ctx)\n",
    "inputs = to_onehot(X.as_in_context(ctx), vocab_size)\n",
    "print(\"input shape: {}\".format(inputs))\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    \"\"\"Predict next chars with a RNN model\"\"\"\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好家日歌铺撑御熬窝林寂刀'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('你好', 11, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Mini-batch stochastic gradient descent.\"\"\"\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0], ctx)\n",
    "        for param in params:\n",
    "            norm += (param.grad ** 2).sum()\n",
    "        norm = norm.sqrt().asscalar()\n",
    "        if norm > theta:\n",
    "            for param in params:\n",
    "                param.grad[:] *= theta / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    \"\"\"Train an RNN model and predict the next item in the sequence.\"\"\"\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:\n",
    "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:\n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            sgd(params, lr, 1)\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(\n",
    "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2, \n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 70.222427, time 0.69 sec\n",
      " - 分开 我想要再生  不知我有多  爱 我不 我不要再想  不知我有多 想爱就我 全小的让我疯狂的可爱女人\n",
      " - 不分开  我有你有  一定我有多 想爱就我 全小的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我\n",
      "epoch 100, perplexity 10.646796, time 0.70 sec\n",
      " - 分开 我想想这样你 不知不觉 你已经这节我 不知不觉 我已经这节我 后知不觉 我已好这节奏 后知后觉 我\n",
      " - 不分开吗 我想你这想你 不知不觉 你已经这节我 不知不觉 我已经这节我 后知不觉 我已好这节奏 后知后觉 \n",
      "epoch 150, perplexity 3.017763, time 0.70 sec\n",
      " - 分开 一颗用双截棍 哼哼哈兮 快使用人太记 仁生无敌 是谁在练太极  哼穿了我 不要一口热 折制茶烛抽 \n",
      " - 不分开吗 我不能再想 我不 我不 我不要再想你 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 \n",
      "epoch 200, perplexity 1.618426, time 0.70 sec\n",
      " - 分开 一颗用双留 谁底它停留的 为什么我女朋友场外加油 你却还让我出糗 从小就耳濡目染 什么刀枪跟棍棒 \n",
      " - 不分开吗 我叫你爸 你打我妈 这样对吗干嘛这样 什么让危险边缘Bab 印地安斑鸠 会学人开口 仙人掌怕羞 \n",
      "epoch 250, perplexity 1.306915, time 0.71 sec\n",
      " - 分开 我想想你 是我面外婆堡  说穿了其实我的愿望就怎么小 就怎么每天祈祷我的心跳你知道  杵在伊斯坦堡\n",
      " - 不分开期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱\n"
     ]
    }
   ],
   "source": [
    "is_random_iter = True\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, ctx,  corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gluon 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_jay_lyrics():\n",
    "    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\n",
    "    with zipfile.ZipFile(data_file) as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256 \n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 256)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "state = rnn_layer.begin_state(batch_size=batch_size)\n",
    "state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 2, 256), 1, (1, 2, 256))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = 35\n",
    "X = nd.random.uniform(shape=(num_steps, batch_size, vocab_size))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, len(state_new), state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    \"\"\"RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Dense(vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = nd.one_hot(inputs.T, self.vocab_size)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.dense(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    \"\"\"Predict next chars with a RNN model\"\"\"\n",
    "    state = model.begin_state(batch_size=1, ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    \n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = nd.array([output[-1]], ctx=ctx).reshape((1,1))\n",
    "        (Y, state) = model(X, state)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes):\n",
    "    \"\"\"Train an Gluon RNN model and predict the next item in the sequence.\"\"\"\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_consecutive(\n",
    "            corpus_indices, batch_size, num_steps, ctx)\n",
    "        state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
    "        for X, Y in data_iter:\n",
    "            for s in state:\n",
    "                s.detach()\n",
    "            with autograd.record():\n",
    "                (output, state) = model(X, state)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(output, y).mean()\n",
    "            l.backward()\n",
    "            params = [p.data() for p in model.collect_params().values()]\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            trainer.step(1)\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_gluon(\n",
    "                    prefix, pred_len, model, vocab_size, ctx, idx_to_char,\n",
    "                    char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开团窜麦珊浩炮書局捡足缝给渲信脂窜麦珊浩炮预侬慈宽烊玄浩讽弯饭展卡刮吾牢扬码岂侬寇'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(rnn_layer, vocab_size)\n",
    "model.initialize(force_reinit=True, ctx=ctx)\n",
    "predict_rnn_gluon('分开' , 40, model, vocab_size, ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_epochs, batch_size, lr, clipping_theta = 700, 32, 1e2, 1e-2\n",
    "\n",
    "pre_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = try_gpu()\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "    \n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)), \n",
    "                _one((num_hiddens, num_hiddens)), \n",
    "                nd.zeros(num_hiddens, ctx=ctx))\n",
    "    \n",
    "    W_xi, W_hi, b_i = _three()\n",
    "    W_xf, W_hf, b_f = _three()\n",
    "    W_xo, W_ho, b_o = _three()\n",
    "    W_xc, W_hc, b_c = _three()\n",
    "    \n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    \n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]\n",
    "    \n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), \n",
    "            nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx))\n",
    "\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n",
    "    \n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    \n",
    "    for X in inputs:\n",
    "        I = nd.sigmoid(nd.dot(X, W_xi)+ nd.dot(H, W_hi)+ b_i)\n",
    "        F = nd.sigmoid(nd.dot(X, W_xf)+ nd.dot(H, W_hf)+ b_f)\n",
    "        O = nd.sigmoid(nd.dot(X, W_xo)+ nd.dot(H, W_ho) +b_o)\n",
    "        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc)+ b_c)\n",
    "        \n",
    "        C = F *C + I * C_tilda\n",
    "        H = O * C.tanh()\n",
    "        \n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "        \n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 1000,35, 32, 1e2, 1e-2\n",
    "\n",
    "pred_period, pred_len, prefixes = 50, 50, ['喜欢', '不分开']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 161.083389, time 1.67 sec\n",
      " - 喜欢 我不的我 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 \n",
      " - 不分开 我想我 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我不不 我\n",
      "epoch 100, perplexity 32.680239, time 1.73 sec\n",
      " - 喜欢 我想你的你笑 有你 你想我的久  有 你的你很久 想想 你想 我不要 我不 我不 我不要 我不要 \n",
      " - 不分开 我想你的爱笑 一样 我想你的你有 有你 你想我想 我想 我想 我不要 我不要 我不要 我不要 我不\n",
      "epoch 150, perplexity 5.245681, time 1.66 sec\n",
      " - 喜欢 我想带你 你不著听 想想就这样着我妈妈 难道你的手快幽默 不要再这样打我妈妈 难道你不了 让让我 \n",
      " - 不分开 我已要你 我不要烦  我有你 说你是是我 我开开这样活着你 别怪开 别怪我 说你怎么 对对怎么停么\n",
      "epoch 200, perplexity 1.730521, time 1.66 sec\n",
      " - 喜欢 我已儿 其子我 一定伦中中对人 双是壁壁中里多 我该儿河我有棒 一天忙人地当 快使用双截棍 哼哼哈\n",
      " - 不分开 我已经这生我 不知不觉 你已经离节奏 后知不觉 我该好好生活 我该好好生活 静静悄悄默默离开 陷入\n",
      "epoch 250, perplexity 1.195605, time 1.67 sec\n",
      " - 喜欢 问弄我 是属于那手代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开 我已经这生我 不知不觉 我跟了这节奏 后知后觉 后知后觉 迷迷蒙蒙 你给的梦 出现裂缝 隐隐作痛 \n",
      "epoch 300, perplexity 1.076256, time 1.72 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开 我已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活\n",
      "epoch 350, perplexity 1.073498, time 1.66 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 400, perplexity 1.044198, time 1.67 sec\n",
      " - 喜欢 问候我 谁是神枪手 巫师 他念念 有词的 对酋长下诅咒 还我骷髅头 这故事 告诉我 印地安的传说 \n",
      " - 不分开 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活\n",
      "epoch 450, perplexity 1.055120, time 1.67 sec\n",
      " - 喜欢 问候我 是属于枪手手巫记 还檐金钟罩铁 店荡的蓝池边 河截棍柔武 当伤透 不数风中 还自彩看的你 \n",
      " - 不分开觉 我已经离想我 要知不觉 你跟经离节活 后知后觉 又过后一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 500, perplexity 1.024818, time 1.69 sec\n",
      " - 喜欢 问候我 是是神枪手代巫墙黑瓦瓦的淡淡 忧伤的外旧 一壶狠酒 娘来一碗蛛粥辛愁 隔堡里国阳的芜  像\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 550, perplexity 1.022692, time 1.67 sec\n",
      " - 喜欢 问弄堂的太呼 闪烁成回忆 伤限的美丽 你的完美主义 太彻底 让我连恨都难以下笔 将真心抽离写成日记\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 600, perplexity 1.019767, time 1.68 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 650, perplexity 1.021839, time 1.67 sec\n",
      " - 喜欢 问候我 是是神枪手代巫巫 还会金钟罩铁步衫 他们儿子我习惯 从小堂跟武当 快使话 干诉大 我想开任\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟经这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 700, perplexity 1.020150, time 1.78 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 750, perplexity 1.017859, time 1.67 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 800, perplexity 1.016175, time 1.69 sec\n",
      " - 喜欢 问弄堂 是属于那年代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 我已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 850, perplexity 1.024181, time 1.72 sec\n",
      " - 喜欢 问候我 是是于枪手代白墙黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 我已经离开我要要不错难么  托道回层 我试著努力向你奔跑  才叫我怎么跟你像 不要再这样打我妈妈\n",
      "epoch 900, perplexity 1.022260, time 1.72 sec\n",
      " - 喜欢 问候我 是是神枪手巫巫巫黑瓦的淡淡的忧伤 消失的 旧时光 一九四三 回头看 的片段 有一些风霜 老\n",
      " - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\n",
      "epoch 950, perplexity 1.013894, time 1.69 sec\n",
      " - 喜欢 问候我 谁是神枪手 巫师 他念念 有词的 对酋长下诅咒 还我骷髅头 这故事 告诉我 印地安的传说 \n",
      " - 不分开 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活\n"
     ]
    }
   ],
   "source": [
    "is_random_iter = False\n",
    "train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                          vocab_size, ctx,  corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mxnet_p36]",
   "language": "python",
   "name": "conda-env-mxnet_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
