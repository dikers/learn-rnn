{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation using NumPy\n",
    " \n",
    "\n",
    "使用numpy 实现一个简单的RNN 网络模型， 用来学习RNN 的前向传播和反向转播过程 . \n",
    "下面的例子是一个多对一问题， 通过前面多个字符，预测后一个字符。 \n",
    "\n",
    "RNN多对一 适合解决文本分类，时间序列的预测问题。 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文档\n",
    "* [循环神经网络RNN介绍](https://zhuanlan.zhihu.com/p/32755043)\n",
    "\n",
    "* [用「动图」和「举例子」讲讲 RNN](https://zhuanlan.zhihu.com/p/36455374)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算原理和公式\n",
    "\n",
    "\n",
    "![image](https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/images/rnn_simple.png)\n",
    "\n",
    "![image](https://pic1.zhimg.com/80/v2-4058db6817f202ddc3fc41cb3683a744_1440w.png)\n",
    "![image](https://pic1.zhimg.com/80/v2-a2ba1b9625a25a9f3c4a16acba069e50_1440w.png)\n",
    "\n",
    "\n",
    "### 步骤一\n",
    "输入网络的第一个字母是“h”, 我们想要得到隐藏层状态，那么首先我们需要计算\n",
    "![image](https://www.zhihu.com/equation?tex=W_%7Bxh%7Dx_t)\n",
    "\n",
    "\n",
    "\n",
    "![image](https://pic2.zhimg.com/80/v2-e97f9dabaeed61846bed910a92638281_1440w.jpg)\n",
    "\n",
    "![image](https://pic1.zhimg.com/80/v2-a5bd70373a49379c155a55f124686134_1440w.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "通过矩阵相乘，我们得到\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.287027 0.902874 0.537524]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "one_hot =[[1, 0,  0, 0],\n",
    "          [0, 1,  0, 0],\n",
    "          [0, 0,  1, 0],\n",
    "          [0, 0,  1, 0]]\n",
    "\n",
    "wxh = [\n",
    "      [0.287027, 0.84606,  0.572392, 0.486813], \n",
    "      [0.902874, 0.871522, 0.691079, 0.18998], \n",
    "      [0.537524, 0.09224,  0.558159, 0.491528]]\n",
    "\n",
    "xt = one_hot[0]\n",
    "r1 = np.dot(wxh, xt)\n",
    "\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 步骤二\n",
    "\n",
    "现在我们看一下循环神经元, 权重[公式]是一个1x1的矩阵，值为0.427043，偏差也是1x1的矩阵，值为0.56700.\n",
    "\n",
    "对于字母“h”，没有前一个状态，所以我们也可以认为前一个状态是[0,0,0,0]。\n",
    "\n",
    "接下来计算\n",
    "\n",
    "![image](https://www.zhihu.com/equation?tex=w_%7Bhh%7Dh_%7Bt-1%7D%2Bbias)\n",
    "\n",
    "\n",
    "![image](https://pic1.zhimg.com/80/v2-34851f8a03663d8d57d96683431e53d0_1440w.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.567001 0.567001 0.567001]\n"
     ]
    }
   ],
   "source": [
    "h_state =[[0, 0,  0, 0],\n",
    "          [0, 0,  0, 0],\n",
    "          [0, 0,  0, 0],\n",
    "          [0, 0,  0, 0]]\n",
    "\n",
    "h_0 = h_state[0]\n",
    "\n",
    "whh = 0.427043\n",
    "bias = 0.567001\n",
    "\n",
    "\n",
    "r2 =  np.dot(whh, h_0) + bias \n",
    "r2 = r2[:-1]\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤三\n",
    "有了前两步，我们就可以计算当前循环神经元的状态了，根据以下公式\n",
    "\n",
    "![image](https://www.zhihu.com/equation?tex=h_t+%3D+tanh%28W_%7Bhh%7Dh_%7Bt-1%7D%2BW_%7Bxh%7Dx_%7Bt%7D%29)\n",
    "\n",
    "将前两步的结果代入公式即可得到当前步的状态，计算如下\n",
    "\n",
    "\n",
    "![image](https://pic2.zhimg.com/80/v2-de56eaf660c4d6bf6341c99ddef037ad_1440w.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.854028 1.469875 1.104525]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.69316794, 0.89955361, 0.80211853])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(r1 + r2)\n",
    "np.tanh(r1 + r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  27\n",
      "txt_data_size :  81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt_data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz \" # input data\n",
    "\n",
    "chars = list(set(txt_data)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(txt_data)\n",
    "\n",
    "print(\"unique characters : \", num_chars) # You can see the number of unique characters in your input data.\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 0, 's': 1, 't': 2, 'n': 3, 'j': 4, 'b': 5, 'x': 6, 'l': 7, 'v': 8, 'm': 9, 'o': 10, 'p': 11, ' ': 12, 'e': 13, 'r': 14, 'y': 15, 'g': 16, 'w': 17, 'a': 18, 'h': 19, 'c': 20, 'i': 21, 'd': 22, 'q': 23, 'f': 24, 'u': 25, 'z': 26}\n",
      "----------------------------------------------------\n",
      "{0: 'k', 1: 's', 2: 't', 3: 'n', 4: 'j', 5: 'b', 6: 'x', 7: 'l', 8: 'v', 9: 'm', 10: 'o', 11: 'p', 12: ' ', 13: 'e', 14: 'r', 15: 'y', 16: 'g', 17: 'w', 18: 'a', 19: 'h', 20: 'c', 21: 'i', 22: 'd', 23: 'q', 24: 'f', 25: 'u', 26: 'z'}\n",
      "----------------------------------------------------\n",
      "[18, 5, 20, 22, 13, 24, 16, 19, 21, 4, 0, 7, 9, 3, 10, 11, 23, 14, 1, 2, 25, 8, 17, 6, 15, 26, 12, 18, 5, 20, 22, 13, 24, 16, 19, 21, 4, 0, 7, 9, 3, 10, 11, 23, 14, 1, 2, 25, 8, 17, 6, 15, 26, 12, 18, 5, 20, 22, 13, 24, 16, 19, 21, 4, 0, 7, 9, 3, 10, 11, 23, 14, 1, 2, 25, 8, 17, 6, 15, 26, 12]\n",
      "----------------------------------------------------\n",
      "data length :  81\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 27)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Not actually used.\n",
    "\n",
    "onehot_encoded = []\n",
    "\n",
    "for ix in integer_encoded: # ix is an index mapped to a unique character.\n",
    "    letter = [0 for _ in range(len(chars))] # A list len is equal to the number of unique characters and whose elements are all zero.\n",
    "    letter[ix] = 1 # 'letter' is a one-hot vector.\n",
    "    onehot_encoded.append(letter) # Add a 1d list(a vector for one character).\n",
    "onehot_encoded = np.array(onehot_encoded) # list to np-array   \n",
    "\n",
    "print(onehot_encoded.shape)     #  = (len(data),len(chars))\n",
    "print(onehot_encoded[0:4])\n",
    "# invert encoding\n",
    "inverted = int_to_char[np.argmax(onehot_encoded[0])] # \"argmax\" returns the index of the largest value. \n",
    "print(inverted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 500\n",
    "sequence_length = 10\n",
    "# batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "batch_size = 4\n",
    "hidden_size = 100  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation\n",
    "\n",
    "![image](https://pic1.zhimg.com/80/v2-4058db6817f202ddc3fc41cb3683a744_1440w.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "       \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    " \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "![image](https://pic4.zhimg.com/80/v2-4518c23614c9208abdffab055ab88333_1440w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation\n",
    "\n",
    "[反向传播 推到过程](https://zhuanlan.zhihu.com/p/79657669)\n",
    "\n",
    "![image](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+out%7D+%5CRightarrow+%5Cleft+%5C%7B%5Cbegin%7Bmatrix%7D+++++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+W_%7BL%2B1%7D%7D%3DZ_L%5ET%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+out%7D+%5C%5C+++++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+Z_%7BL%7D%7D%3D%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+out%7DW_%7BL%2B1%7D%5ET+%5C%5C+++++%5Cleft%28%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+b%7D%5Cright%29%5E%7BT%7D%3DSumCol%28%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+out%7D%29+%5C%5C+++++W_%7BL%2B1%7D%5E%7Bt%2B1%7D+%3D+W_%7BL%2B1%7D%5Et-%5Ceta+%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+W_%7BL%2B1%7D%7D+%5C%5C+++++b_%7BL%2B1%7D%5E%7Bt%2B1%7D+%3D+b_%7BL%2B1%7D%5Et-%5Ceta+%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+b_%7BL%2B1%7D%7D+%5Cend%7Bmatrix%7D+%5Cright.+%5CRightarrow+%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+h_L%7D%3D%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+Z_L%7D%5Cfrac%7B%5Cpartial+Z_L%7D%7B%5Cpartial+h_L%7D+%5CRightarrow+%5Cleft+%5C%7B%5Cbegin%7Bmatrix%7D++++++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+W_%7BL%7D%7D%3DZ_%7BL-1%7D%5ET%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+h_L%7D+%5C%5C+++++%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+Z_%7BL-1%7D%7D%3D%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+h_L%7DW_%7BL%7D%5ET+%5C%5C+++++%5Cvdots+%5C%5C+++++%5Cvdots++%5Cend%7Bmatrix%7D%5Cright.+%5CRightarrow+%5Ccdots+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        \n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 5, 20, 22, 13, 24, 16, 19, 21, 4]\n",
      "iter 0, loss: 61.347369\n",
      "iter 200, loss: 0.046441\n",
      "iter 400, loss: 0.019800\n"
     ]
    }
   ],
   "source": [
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    # batch_size 没有并行计算， 如果用tf， 可以并行计算\n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "  \n",
    "        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " bcdefghijklmnopqrstuvwxyz abcd \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('a',30) # (char, len of output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " cdefghijklmnopqrstuvwxyz abcde \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('b',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
