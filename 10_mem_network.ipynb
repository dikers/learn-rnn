{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  1.14.0\n",
      "GPU :  True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from functools import reduce\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "print('tf version: ', tf.__version__)\n",
    "print('GPU : ', tf.test.is_gpu_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('./', 'datasets')\n",
    "def get_file(filename, url=None, datadir=None):\n",
    "    if url is None:\n",
    "        raise\n",
    "    if datadir is None:\n",
    "        datadir = base_dir\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    fpath = os.path.join(datadir, filename)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        pass\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        print('Downloading data from', url)\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(url, fpath)\n",
    "            except URLError as e:\n",
    "                raise\n",
    "            except HTTPError as e:\n",
    "                raise\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    return fpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, q, n_batch,\n",
    "              vocab_size=None,\n",
    "              embedding_dim=None,\n",
    "              story_maxlen=None,\n",
    "              question_maxlen=None):\n",
    "    def weight_variable(shape, stddev=0.08):\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.zeros(shape, dtype=tf.float32)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    A = weight_variable([vocab_size, embedding_dim])\n",
    "    B = weight_variable([vocab_size, embedding_dim])\n",
    "    C = weight_variable([vocab_size, question_maxlen])\n",
    "    m = tf.nn.embedding_lookup(A, x)\n",
    "    u = tf.nn.embedding_lookup(B, q)\n",
    "    c = tf.nn.embedding_lookup(C, x)\n",
    "    p = tf.nn.softmax(tf.einsum('ijk,ilk->ijl', m, u))\n",
    "    o = tf.add(p, c)\n",
    "    o = tf.transpose(o, perm=[0, 2, 1])\n",
    "    ou = tf.concat([o, u], axis=-1)\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(embedding_dim//2, forget_bias=1.0)\n",
    "    initial_state = cell.zero_state(n_batch, tf.float32)\n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    with tf.variable_scope('LSTM'):\n",
    "        for t in range(question_maxlen):\n",
    "            if t > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            (cell_output, state) = cell(ou[:, t, :], state)\n",
    "            outputs.append(cell_output)\n",
    "    output = outputs[-1]\n",
    "    W = weight_variable([embedding_dim//2, vocab_size], stddev=0.01)\n",
    "    a = tf.nn.softmax(tf.matmul(output, W))\n",
    "\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(y, t):\n",
    "    cross_entropy = \\\n",
    "        tf.reduce_mean(-tf.reduce_sum(\n",
    "                       t * tf.log(tf.clip_by_value(y, 1e-10, 1.0)),\n",
    "                       reduction_indices=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def accuracy(y, t):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, max_length=None):\n",
    "    def flatten(data):\n",
    "        return reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    data = parse_stories(f.readlines())\n",
    "    data = [(flatten(story), q, answer)\n",
    "            for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_indices, story_maxlen, question_maxlen):\n",
    "    X = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, question, answer in data:\n",
    "        x = [word_indices[w] for w in story]\n",
    "        q = [word_indices[w] for w in question]\n",
    "        a = np.zeros(len(word_indices) + 1)   \n",
    "        a[word_indices[answer]] = 1\n",
    "        X.append(x)\n",
    "        Q.append(q)\n",
    "        A.append(a)\n",
    "\n",
    "    return (padding(X, maxlen=story_maxlen),\n",
    "            padding(Q, maxlen=question_maxlen), np.array(A))\n",
    "\n",
    "\n",
    "def padding(words, maxlen):\n",
    "    for i, word in enumerate(words):\n",
    "        words[i] = [0] * (maxlen - len(word)) + word\n",
    "    return np.array(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Fetching data...')\n",
    "try:\n",
    "    path = \\\n",
    "        get_file('babi-tasks-v1-2.tar.gz',\n",
    "                 url='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except Exception as e:\n",
    "    raise\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n"
     ]
    }
   ],
   "source": [
    "train_stories[0]\n",
    "print(train_stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story_maxlen  68\n",
      "question_maxlen  4\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1   \n",
    "\n",
    "story_maxlen = \\\n",
    "    max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "question_maxlen = \\\n",
    "    max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"story_maxlen \", story_maxlen)\n",
    "print(\"question_maxlen \", question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vectorizing data...')\n",
    "word_indices = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, questions_train, answers_train = \\\n",
    "    vectorize_stories(train_stories, word_indices,\n",
    "                      story_maxlen, question_maxlen)\n",
    "\n",
    "inputs_test, questions_test, answers_test = \\\n",
    "    vectorize_stories(test_stories, word_indices,\n",
    "                      story_maxlen, question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.', 'Sandra', 'moved', 'to', 'the', 'garden', '.'], ['Where', 'is', 'Daniel', '?'], 'hallway')\n",
      "10000\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(train_stories[1])\n",
    "print(len(train_stories))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-1fde4ef9bf20>:25: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training model...\n",
      "epoch:     0  val_loss: 1.82578  val_acc: 0.18200\n",
      "epoch:    50  val_loss: 1.62779  val_acc: 0.26600\n",
      "epoch:   100  val_loss: 0.25728  val_acc: 0.92100\n",
      "epoch:   150  val_loss: 0.19174  val_acc: 0.95600\n",
      "epoch:   200  val_loss: 0.23963  val_acc: 0.96000\n",
      "epoch:   250  val_loss: 0.27442  val_acc: 0.96100\n",
      "epoch:   300  val_loss: 0.29315  val_acc: 0.96100\n",
      "epoch:   350  val_loss: 0.30191  val_acc: 0.96500\n",
      "epoch:   400  val_loss: 0.25562  val_acc: 0.96400\n",
      "epoch:   450  val_loss: 0.27440  val_acc: 0.96600\n",
      "epoch:   500  val_loss: 0.28064  val_acc: 0.96800\n",
      "epoch:   550  val_loss: 0.28184  val_acc: 0.96900\n",
      "epoch:   600  val_loss: 0.29053  val_acc: 0.96600\n",
      "epoch:   650  val_loss: 0.28377  val_acc: 0.96600\n",
      "epoch:   700  val_loss: 0.18618  val_acc: 0.96800\n",
      "epoch:   750  val_loss: 0.20512  val_acc: 0.97100\n",
      "epoch:   800  val_loss: 0.24041  val_acc: 0.97100\n",
      "epoch:   850  val_loss: 0.26761  val_acc: 0.97600\n",
      "epoch:   900  val_loss: 0.27221  val_acc: 0.97400\n",
      "epoch:   950  val_loss: 0.26651  val_acc: 0.97400\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "print('Building model...')\n",
    "x = tf.placeholder(tf.int32, shape=[None, story_maxlen])\n",
    "q = tf.placeholder(tf.int32, shape=[None, question_maxlen])\n",
    "a = tf.placeholder(tf.float32, shape=[None, vocab_size])\n",
    "n_batch = tf.placeholder(tf.int32, shape=[])\n",
    "\n",
    "y = inference(x, q, n_batch,\n",
    "              vocab_size=vocab_size,\n",
    "              embedding_dim=64,\n",
    "              story_maxlen=story_maxlen,\n",
    "              question_maxlen=question_maxlen)\n",
    "loss_ = loss(y, a)\n",
    "train_step = training(loss_)\n",
    "acc = accuracy(y, a)\n",
    "history = {\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "n_batches = len(inputs_train) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs_train_, questions_train_, answers_train_ = \\\n",
    "        shuffle(inputs_train, questions_train, answers_train)\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: inputs_train_[start:end],\n",
    "            q: questions_train_[start:end],\n",
    "            a: answers_train_[start:end],\n",
    "            n_batch: batch_size\n",
    "        })\n",
    "\n",
    "    val_loss = loss_.eval(session=sess, feed_dict={\n",
    "        x: inputs_test,\n",
    "        q: questions_test,\n",
    "        a: answers_test,\n",
    "        n_batch: len(inputs_test)\n",
    "    })\n",
    "    val_acc = acc.eval(session=sess, feed_dict={\n",
    "        x: inputs_test,\n",
    "        q: questions_test,\n",
    "        a: answers_test,\n",
    "        n_batch: len(inputs_test)\n",
    "    })\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    if epoch % 50 ==0: \n",
    "        print('epoch: {:5d}  val_loss: {:.5f}  val_acc: {:.5f}'\n",
    "              .format(epoch, val_loss , val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
