{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类\n",
    "\n",
    "*  使用循环神经网络对文本进行分类, 熟悉多对一问题处理\n",
    "*  熟悉中文NLP处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import jieba # pip install jieba\n",
    "import urllib.request as ur\n",
    "\n",
    "data_file_url = 'https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/dataset/cnews_data.zip'\n",
    "base_dir = './dataset/cnews/'\n",
    "\n",
    "# input files\n",
    "train_file = base_dir + 'cnews.train.txt'\n",
    "val_file = base_dir + 'cnews.val.txt'\n",
    "test_file = base_dir + 'cnews.test.txt'\n",
    "\n",
    "# output files\n",
    "seg_train_file = base_dir + 'cnews.train.seg.txt'\n",
    "seg_val_file = base_dir + 'cnews.val.seg.txt'\n",
    "seg_test_file = base_dir + 'cnews.test.seg.txt'\n",
    "\n",
    "vocab_file = base_dir + 'cnews.vocab.txt'\n",
    "category_file = base_dir + 'cnews.category.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "数据格式  \n",
    "  分类  文本详细内容\n",
    ">体育    -->  黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之......\n",
    "\n",
    ">娱乐    -->  深湖巨兽 导演想来中国拍“水怪”晚报讯 由华夏电影公司发行的电影.....\n",
    "\n",
    ">家居    -->  橱柜的商品性质十分模糊(二)橱柜到底是什么性质的商品十分模糊：.....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdf/workspace/git_hub_demo/learn-rnn\n",
      "mkdir: cannot create directory ‘./datasets’: File exists\n",
      "./dataset/cnews/cnews.train.txt不存在, 开始下载文件\n",
      "Archive:  ./cnews_data.zip\n",
      "  inflating: cnews.test.txt          \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._cnews.test.txt  \n",
      "  inflating: cnews.val.txt           \n",
      "  inflating: __MACOSX/._cnews.val.txt  \n",
      "  inflating: cnews.train.txt         \n",
      "  inflating: __MACOSX/._cnews.train.txt  \n",
      "mkdir: cannot create directory ‘./datasets/cnews’: File exists\n",
      "CPU times: user 409 ms, sys: 253 ms, total: 662 ms\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pwd\n",
    "if not os.path.exists(train_file):\n",
    "    !mkdir ./datasets\n",
    "    !mkdir ./datasets/cnews\n",
    "    print('{}不存在, 开始下载文件'.format(train_file))\n",
    "    ur.urlretrieve(data_file_url, \"cnews_data.zip\")\n",
    "    !unzip ./cnews_data.zip\n",
    "    !rm ./cnews_data.zip\n",
    "    !mkdir ./datasets/cnews \n",
    "    !mv cnews.train.txt ./datasets/cnews/\n",
    "    !mv cnews.test.txt ./datasets/cnews/\n",
    "    !mv cnews.val.txt ./datasets/cnews/\n",
    "    !rm -fr __MACOSX\n",
    "else:\n",
    "    print('文件已经存在')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用jieba分词\n",
    "\n",
    "如果没有安装jieba 请安装， 命令如下： \n",
    "\n",
    "```\n",
    "conda info --envs\n",
    "\n",
    "source activate tensorflow_p36\n",
    "\n",
    "pip install jieba\n",
    "\n",
    "```\n",
    "然后重新启动kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 体育\n",
      "黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题][黄蜂vs湖人图文直播室](新浪体育)\n",
      "黄蜂/ vs/ 湖人/ 首发/ ：/ 科比/ 带伤/ 战/ 保罗/  / 加索尔/ 救赎/ 之战/  / 新浪/ 体育讯/ 北京/ 时间/ 4/ 月/ 27/ 日/ ，/ NBA/ 季后赛/ 首轮/ 洛杉矶/ 湖人/ 主场/ 迎战/ 新奥尔良/ 黄蜂/ ，/ 此前/ 的/ 比赛/ 中/ ，/ 双方/ 战成/ 2/ -/ 2/ 平/ ，/ 因此/ 本场/ 比赛/ 对于/ 两支/ 球队/ 来说/ 都/ 非常/ 重要/ ，/ 赛前/ 双方/ 也/ 公布/ 了/ 首发/ 阵容/ ：/ 湖人队/ ：/ 费舍尔/ 、/ 科比/ 、/ 阿泰斯特/ 、/ 加索尔/ 、/ 拜纳姆/ 黄蜂队/ ：/ 保罗/ 、/ 贝里/ 内利/ 、/ 阿里/ 扎/ 、/ 兰德/ 里/ 、/ 奥卡福/ [/ 新浪/ NBA/ 官方/ 微博/ ]/ [/ 新浪/ NBA/ 湖人/ 新闻动态/ 微博/ ]/ [/ 新浪/ NBA/ 专题/ ]/ [/ 黄蜂/ vs/ 湖人/ 图文/ 直播室/ ]/ (/ 新浪/ 体育/ )\n"
     ]
    }
   ],
   "source": [
    "# 演示分词的用法\n",
    "with open(val_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "word_iter = jieba.cut(content)\n",
    "\n",
    "print('label', label)\n",
    "print(content)\n",
    "print('/ '.join(word_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  将样本文件分词\n",
    "\n",
    "分词后文件格式\n",
    "\n",
    ">体育-->黄蜂 vs 湖人 首发 ： 科比 带伤 战 保罗 加索尔 救赎 之战\n",
    "\n",
    ">体育-->广东 半场 狂飙 狂胜 山东 陈 江华 关键 闪光 稳住 全队 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def generate_seg_file(input_file, output_seg_file):\n",
    "    \"\"\"Segment the sentences in each line in input_file\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_seg_file, 'w',  encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in word_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != '':\n",
    "                    word_content += word + ' '\n",
    "            out_line = '%s\\t%s\\n' % (label, word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "        print('{} 文件分割完成, 输出路径{} .'.format(input_file, output_seg_file))\n",
    "        \n",
    "if not os.path.exists(seg_train_file):\n",
    "    generate_seg_file(train_file, seg_train_file)\n",
    "if not os.path.exists(seg_val_file):\n",
    "    generate_seg_file(val_file, seg_val_file)\n",
    "if not os.path.exists(seg_test_file):\n",
    "    generate_seg_file(test_file, seg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词汇表\n",
    "\n",
    "\n",
    "| 词语 | 出现的数量 |\n",
    "| :-----| ----: |\n",
    "| 生活 | 13141 |\n",
    "| 能够 | 12911 |\n",
    "| 不会 | 12898 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.63 s, sys: 76.2 ms, total: 6.7 s\n",
      "Wall time: 6.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def generate_vocab_file(input_seg_file, output_vocab_file):\n",
    "    with open(input_seg_file, 'r',  encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            word_dict.setdefault(word, 0)\n",
    "            word_dict[word] += 1\n",
    "    # [(word, frequency), ..., ()]\n",
    "    sorted_word_dict = sorted(\n",
    "        word_dict.items(), key = lambda d:d[1], reverse=True)\n",
    "    with open(output_vocab_file, 'w',  encoding='utf-8') as f:\n",
    "        f.write('<UNK>\\t10000000\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % (item[0], item[1]))\n",
    "\n",
    "generate_vocab_file(seg_train_file, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab file 格式： \n",
    "词语 和 出现的数量\n",
    "\n",
    "```\n",
    "生活\t13141\n",
    "能够\t12911\n",
    "不会\t12898\n",
    "不同\t12871\n",
    "获得\t12870\n",
    "城市\t12825\n",
    "学校\t12775\n",
    "一定\t12736\n",
    "一直\t12606\n",
    "上海\t12574\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对应的label 标签\n",
    "\n",
    "\n",
    "| 分类标签 | 出现的数量 |\n",
    "| :-----| ----: |\n",
    "| 体育 | 5000 |\n",
    "| 娱乐 | 5000 |\n",
    "| 家居 | 5000 |\n",
    "| 房产 | 5000 |\n",
    "| 时尚 | 5000 |\n",
    "| 时政 | 5000 |\n",
    "| 游戏 | 5000 |\n",
    "| 科技 | 5000 |\n",
    "| 财经 | 5000 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t5000\n",
      "娱乐\t5000\n",
      "家居\t5000\n",
      "房产\t5000\n",
      "教育\t5000\n",
      "时尚\t5000\n",
      "时政\t5000\n",
      "游戏\t5000\n",
      "科技\t5000\n",
      "财经\t5000\n",
      "CPU times: user 348 ms, sys: 29.6 ms, total: 378 ms\n",
      "Wall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def generate_category_dict(input_file, category_file):\n",
    "    with open(input_file, 'r',  encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        category_dict.setdefault(label, 0)\n",
    "        category_dict[label] += 1\n",
    "    category_number = len(category_dict)\n",
    "    with open(category_file, 'w',  encoding='utf-8') as f:\n",
    "        for category in category_dict:\n",
    "            line = '%s\\n' % category\n",
    "            print('%s\\t%d' % (category, category_dict[category]))\n",
    "            f.write(line)\n",
    "            \n",
    "generate_category_dict(train_file, category_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 77323\n",
      "num_classes: 10\n",
      "label: 时尚, id: 5\n",
      "CPU times: user 218 ms, sys: 10.2 ms, total: 228 ms\n",
      "Wall time: 227 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_word_threshold = 10\n",
    "num_timesteps = 50\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r',  encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    def get_word_dict(self):\n",
    "        return self._word_to_id\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r',  encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Execption(\n",
    "                \"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "print('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "print('num_classes: %d' % num_classes)\n",
    "test_str = '时尚'\n",
    "print(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77323\n",
      "total_count 77323\n",
      "0.03590134888713578\n",
      "0.9120665058006028\n",
      "CPU times: user 140 ms, sys: 2.86 ms, total: 143 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "import numpy as np\n",
    "word_counts = vocab.get_word_dict()\n",
    "threshold = 1e-5\n",
    "print(len(word_counts))\n",
    "total_count = len(word_counts)\n",
    "print('total_count', total_count)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "print(freqs['儿童'])\n",
    "# p_drop = {word: 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "p_drop = {vocab.word_to_id(word): 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "print(p_drop[100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s ./dataset/cnews/cnews.train.seg.txt\n",
      "Loading data from %s ./dataset/cnews/cnews.val.seg.txt\n",
      "Loading data from %s ./dataset/cnews/cnews.test.seg.txt\n",
      "CPU times: user 23.7 s, sys: 81.6 ms, total: 23.8 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        print('Loading data from %s', filename)\n",
    "        with open(filename, 'r',  encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            # 过滤掉一部分概率比较低的值\n",
    "#             print(len(content) , len(id_words))\n",
    "            id_words = [word for word in id_words if random.random() > (1 - p_drop[word])]\n",
    "\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "    \n",
    "    def get_data(self):\n",
    "        batch_inputs = self._inputs\n",
    "        batch_outputs = self._outputs\n",
    "        return batch_inputs, batch_outputs\n",
    "        \n",
    "            \n",
    "train_dataset = TextDataSet(\n",
    "    seg_train_file, vocab, category_vocab,num_timesteps) \n",
    "val_dataset = TextDataSet(\n",
    "    seg_val_file, vocab, category_vocab, num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    seg_test_file, vocab, category_vocab, num_timesteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n",
      "10000\n",
      "(50000, 50)\n",
      "(5000, 50)\n",
      "(10000, 50)\n",
      "y_train.shape (50000, 10)\n",
      "[[  467    11 48363  6409  6162 10443 44435  8019    87  3833  2320    11\n",
      "     49  2335   310 17921   251  2637 35252 25975  4605    50 18271     2\n",
      "    194    91 41216 74378 13858 58863     2 31817 48363  6409  1500     7\n",
      "     31    73   104   376  2528     8  2177   313     3  2598 24072  1808\n",
      "   1335  2923]\n",
      " [ 9433 25390   672 23055 14845 71763   690   301     4   660  7860     2\n",
      "   1791  9433 25390   672 32826   133    32 23055  6816   404   949  2488\n",
      "      6  1237 18224 61989 18167  4238   334   174    44  9433 25390   672\n",
      "  23055 11260    84     6 59349     8    25    38  2244   385 23055     2\n",
      "   4685   469]\n",
      " [ 2358 54606   746   709   648   297  8575   462   709    11  1465   648\n",
      "    962  3267    28  1241   684    59   495   193   746  1465   648 11032\n",
      "    255     6  4808  5930    28   193    86  1728     2  2225    43  1330\n",
      "     13  1161 12374   677     7  1874   962  1496  1733  1176     5  3033\n",
      "    393 17100]]\n",
      "<class '__main__.Vocab'>\n",
      "CPU times: user 3.19 ms, sys: 2 ms, total: 5.19 ms\n",
      "Wall time: 3.36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.utils import to_categorical\n",
    "print(train_dataset.num_examples())\n",
    "print(val_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "x_train , y_train = train_dataset.get_data()\n",
    "x_val, y_val = val_dataset.get_data()\n",
    "x_test, y_test  = test_dataset.get_data()\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print(x_train[5:8])\n",
    "print(type(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "maxlen = x_test.shape[1]\n",
    "_epochs = 10\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "# We add the classifier on top\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_epochs = 10\n",
    "epochs = range(1, _epochs+1)\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(epochs, val_loss, 'b+', color='r',label='Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
